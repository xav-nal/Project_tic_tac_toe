{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "list_of_action = {\n",
    "    0: (0,0),\n",
    "    1: (0,1),\n",
    "    2: (0,2),\n",
    "    3: (1,0),\n",
    "    4: (1,1),\n",
    "    5: (1,2),\n",
    "    6: (2,0),\n",
    "    7: (2,1),\n",
    "    8: (2,2)\n",
    "}\n",
    "\n",
    "rev_action = {\n",
    "    (0,0) :0,\n",
    "    (0,1) :1,\n",
    "    (0,2) :2,\n",
    "    (1,0) :3,\n",
    "    (1,1) :4,\n",
    "    (1,2) :5,\n",
    "    (2,0) :6,\n",
    "    (2,1) :7,\n",
    "    (2,2) :8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_aval_actions(state_of_game,act):\n",
    "  possible_actions = []\n",
    "  for i in range(9):\n",
    "    if state_of_game[act[i]] == 0:\n",
    "      possible_actions.append(i)\n",
    "  return possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, outputs, action_list):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.action_list = action_list\n",
    "        \n",
    "        \n",
    "        self.head1 = nn.Linear(in_features, 128)\n",
    "        self.head2 = nn.Linear(128,128)\n",
    "        self.outputlayer = nn.Linear(128,outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        #print('input x', x.shape, type(x))\n",
    "        x = F.relu(self.head1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.head2(x))\n",
    "        x = self.outputlayer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        #mask_possible_action = torch.ne(obs_tensor,0)*-1\n",
    "        #print('mask_possible_action',mask_possible_action)\n",
    "        \n",
    "        q_values = self.forward(obs_tensor.unsqueeze(0))\n",
    "        #possible_q_values = mask_possible_action*q_values\n",
    "        #print('possible_q_values',possible_q_values)\n",
    "        \n",
    "        \n",
    "        max_q_idx = torch.argmax(q_values, dim=1)[0]\n",
    "        max_q_idx_item = max_q_idx.detach().item()\n",
    "        #print('max_q_idx_item', max_q_idx_item, type(max_q_idx_item))\n",
    "        #print('action list', self.action_list)\n",
    "        #print('rev_action',rev_action)\n",
    "        move = self.action_list[max_q_idx_item]\n",
    "        \n",
    "        action_idx = rev_action[move]\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE_FREQ = 500\n",
    "\n",
    "number_of_actions = 9\n",
    "grid_size = 9\n",
    "\n",
    "\n",
    "env = TictactoeEnv()\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "rew_buffer = deque([0.0], maxlen=10)\n",
    "\n",
    "episode_reward = 0.0\n",
    "\n",
    "#initialization network\n",
    "policy_net = DQN(grid_size, number_of_actions,list_of_action).to(\"cpu\")\n",
    "target_net = DQN(grid_size, number_of_actions,list_of_action).to(\"cpu\")\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "#target_net.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 1 buffer 0\n",
      "game 2 buffer 5\n",
      "game 3 buffer 10\n",
      "game 4 buffer 15\n",
      "game 5 buffer 19\n",
      "game 6 buffer 24\n",
      "game 7 buffer 28\n",
      "game 8 buffer 32\n",
      "game 9 buffer 36\n",
      "game 10 buffer 39\n",
      "game 11 buffer 43\n",
      "game 12 buffer 47\n",
      "game 13 buffer 51\n",
      "game 14 buffer 56\n",
      "game 15 buffer 61\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "MIN_REPLAY_SIZE = 10000\n",
    "\n",
    "#Initialize the Replay Buffer\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "game = 0\n",
    "episode_reward = 0.0\n",
    "\n",
    "while(len(replay_buffer) < BATCH_SIZE):\n",
    "    game += 1\n",
    "    print('game', game, 'buffer', len(replay_buffer))\n",
    "    env.reset()\n",
    "    #env.render()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=1, player='O')\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player='X')\n",
    "    \n",
    "    for j in range(5):\n",
    "        #print('step', j)\n",
    "       \n",
    "        #random agent play\n",
    "        obs = env.observe()[0].flatten()\n",
    "        move = player_rnd.act(grid)\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        action = rev_action[move]\n",
    "        #print('action', action)\n",
    "        \n",
    "        if not end:\n",
    "            #optimal player play\n",
    "            move = player_opt.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "        \n",
    "        new_obs = env.observe()[0].flatten()\n",
    "       \n",
    "    \n",
    "        #transition set-off\n",
    "        rew = env.reward(player=Turns[1])\n",
    "        transition = (obs, action, rew, end, new_obs)\n",
    "        replay_buffer.append(transition)\n",
    "           \n",
    "\n",
    "        if end:\n",
    "            #print('-------------------------------------------')\n",
    "            #print('Game end, winner is player ' + str(winner))\n",
    "            #print('Optimal player = ' +  Turns[0])\n",
    "            #print('Random player = ' +  Turns[1])\n",
    "            #env.render()\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for _ in range(MIN_REPLAY_SIZE):\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(replay_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 2, 0, False, array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., -1.]))\n"
     ]
    }
   ],
   "source": [
    "print(replay_buffer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - -|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n",
      "\n",
      "game 10\n",
      "episode random 30.0\n",
      "Avg random 15.0\n",
      "episode bad predi 5.0\n",
      "Avg bad predi 2.5\n",
      "episode rew -8.0\n",
      "Avg Rew -4.0\n",
      "Loss tensor(0.0410, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 20\n",
      "episode random 28.0\n",
      "Avg random 19.333333333333332\n",
      "episode bad predi 4.0\n",
      "Avg bad predi 3.0\n",
      "episode rew -3.0\n",
      "Avg Rew -3.6666666666666665\n",
      "Loss tensor(0.0403, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 30\n",
      "episode random 14.0\n",
      "Avg random 18.0\n",
      "episode bad predi 4.0\n",
      "Avg bad predi 3.25\n",
      "episode rew -8.0\n",
      "Avg Rew -4.75\n",
      "Loss tensor(0.0528, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 40\n",
      "episode random 16.0\n",
      "Avg random 17.6\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 4.0\n",
      "episode rew -7.0\n",
      "Avg Rew -5.2\n",
      "Loss tensor(0.0516, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 50\n",
      "episode random 3.0\n",
      "Avg random 15.166666666666666\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 4.833333333333333\n",
      "episode rew -8.0\n",
      "Avg Rew -5.666666666666667\n",
      "Loss tensor(0.0751, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 60\n",
      "episode random 2.0\n",
      "Avg random 13.285714285714286\n",
      "episode bad predi 6.0\n",
      "Avg bad predi 5.0\n",
      "episode rew -6.0\n",
      "Avg Rew -5.714285714285714\n",
      "Loss tensor(0.0651, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 70\n",
      "episode random 3.0\n",
      "Avg random 12.0\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 5.25\n",
      "episode rew -8.0\n",
      "Avg Rew -6.0\n",
      "Loss tensor(0.0404, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 80\n",
      "episode random 0.0\n",
      "Avg random 10.666666666666666\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 5.666666666666667\n",
      "episode rew -10.0\n",
      "Avg Rew -6.444444444444445\n",
      "Loss tensor(0.0482, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 90\n",
      "episode random 0.0\n",
      "Avg random 9.6\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 6.1\n",
      "episode rew -10.0\n",
      "Avg Rew -6.8\n",
      "Loss tensor(0.0457, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 100\n",
      "episode random 2.0\n",
      "Avg random 8.909090909090908\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 6.363636363636363\n",
      "episode rew -8.0\n",
      "Avg Rew -6.909090909090909\n",
      "Loss tensor(0.0340, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 110\n",
      "episode random 3.0\n",
      "Avg random 8.416666666666666\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 6.416666666666667\n",
      "episode rew -8.0\n",
      "Avg Rew -7.0\n",
      "Loss tensor(0.0338, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 120\n",
      "episode random 3.0\n",
      "Avg random 8.0\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 6.6923076923076925\n",
      "episode rew -10.0\n",
      "Avg Rew -7.230769230769231\n",
      "Loss tensor(0.0194, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 130\n",
      "episode random 1.0\n",
      "Avg random 7.5\n",
      "episode bad predi 5.0\n",
      "Avg bad predi 6.571428571428571\n",
      "episode rew -6.0\n",
      "Avg Rew -7.142857142857143\n",
      "Loss tensor(0.0261, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 140\n",
      "episode random 0.0\n",
      "Avg random 7.0\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 6.6\n",
      "episode rew -10.0\n",
      "Avg Rew -7.333333333333333\n",
      "Loss tensor(0.0223, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 150\n",
      "episode random 2.0\n",
      "Avg random 6.6875\n",
      "episode bad predi 5.0\n",
      "Avg bad predi 6.5\n",
      "episode rew -9.0\n",
      "Avg Rew -7.4375\n",
      "Loss tensor(0.0227, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 160\n",
      "episode random 1.0\n",
      "Avg random 6.352941176470588\n",
      "episode bad predi 6.0\n",
      "Avg bad predi 6.470588235294118\n",
      "episode rew -8.0\n",
      "Avg Rew -7.470588235294118\n",
      "Loss tensor(0.0375, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 170\n",
      "episode random 1.0\n",
      "Avg random 6.055555555555555\n",
      "episode bad predi 5.0\n",
      "Avg bad predi 6.388888888888889\n",
      "episode rew -6.0\n",
      "Avg Rew -7.388888888888889\n",
      "Loss tensor(0.0396, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 180\n",
      "episode random 4.0\n",
      "Avg random 5.947368421052632\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 6.157894736842105\n",
      "episode rew 0.0\n",
      "Avg Rew -7.0\n",
      "Loss tensor(0.0314, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 190\n",
      "episode random 1.0\n",
      "Avg random 5.7\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 6.0\n",
      "episode rew 0.0\n",
      "Avg Rew -6.65\n",
      "Loss tensor(0.0383, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 200\n",
      "episode random 2.0\n",
      "Avg random 5.523809523809524\n",
      "episode bad predi 4.0\n",
      "Avg bad predi 5.904761904761905\n",
      "episode rew 0.0\n",
      "Avg Rew -6.333333333333333\n",
      "Loss tensor(0.0306, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 210\n",
      "episode random 2.0\n",
      "Avg random 5.363636363636363\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 5.7727272727272725\n",
      "episode rew -4.0\n",
      "Avg Rew -6.2272727272727275\n",
      "Loss tensor(0.0339, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 220\n",
      "episode random 0.0\n",
      "Avg random 5.130434782608695\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 5.565217391304348\n",
      "episode rew 6.0\n",
      "Avg Rew -5.695652173913044\n",
      "Loss tensor(0.0218, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 230\n",
      "episode random 3.0\n",
      "Avg random 5.041666666666667\n",
      "episode bad predi 4.0\n",
      "Avg bad predi 5.5\n",
      "episode rew 0.0\n",
      "Avg Rew -5.458333333333333\n",
      "Loss tensor(0.0272, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 240\n",
      "episode random 0.0\n",
      "Avg random 4.84\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 5.4\n",
      "episode rew 2.0\n",
      "Avg Rew -5.16\n",
      "Loss tensor(0.0287, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 250\n",
      "episode random 3.0\n",
      "Avg random 4.769230769230769\n",
      "episode bad predi 4.0\n",
      "Avg bad predi 5.346153846153846\n",
      "episode rew 0.0\n",
      "Avg Rew -4.961538461538462\n",
      "Loss tensor(0.0632, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 260\n",
      "episode random 3.0\n",
      "Avg random 4.703703703703703\n",
      "episode bad predi 4.0\n",
      "Avg bad predi 5.296296296296297\n",
      "episode rew -5.0\n",
      "Avg Rew -4.962962962962963\n",
      "Loss tensor(0.0250, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 270\n",
      "episode random 3.0\n",
      "Avg random 4.642857142857143\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 5.107142857142857\n",
      "episode rew 5.0\n",
      "Avg Rew -4.607142857142857\n",
      "Loss tensor(0.0283, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 280\n",
      "episode random 1.0\n",
      "Avg random 4.517241379310345\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 4.931034482758621\n",
      "episode rew 4.0\n",
      "Avg Rew -4.310344827586207\n",
      "Loss tensor(0.0164, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 290\n",
      "episode random 1.0\n",
      "Avg random 4.4\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 4.833333333333333\n",
      "episode rew 6.0\n",
      "Avg Rew -3.966666666666667\n",
      "Loss tensor(0.0497, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 300\n",
      "episode random 0.0\n",
      "Avg random 4.258064516129032\n",
      "episode bad predi 4.0\n",
      "Avg bad predi 4.806451612903226\n",
      "episode rew 0.0\n",
      "Avg Rew -3.838709677419355\n",
      "Loss tensor(0.0563, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 310\n",
      "episode random 0.0\n",
      "Avg random 4.125\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 4.6875\n",
      "episode rew 6.0\n",
      "Avg Rew -3.53125\n",
      "Loss tensor(0.0514, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 320\n",
      "episode random 5.0\n",
      "Avg random 4.151515151515151\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 4.606060606060606\n",
      "episode rew 4.0\n",
      "Avg Rew -3.303030303030303\n",
      "Loss tensor(0.0315, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 330\n",
      "episode random 0.0\n",
      "Avg random 4.029411764705882\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 4.470588235294118\n",
      "episode rew 6.0\n",
      "Avg Rew -3.0294117647058822\n",
      "Loss tensor(0.0358, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 340\n",
      "episode random 0.0\n",
      "Avg random 3.914285714285714\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 4.4\n",
      "episode rew 4.0\n",
      "Avg Rew -2.8285714285714287\n",
      "Loss tensor(0.0309, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 350\n",
      "episode random 0.0\n",
      "Avg random 3.8055555555555554\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 4.333333333333333\n",
      "episode rew 6.0\n",
      "Avg Rew -2.5833333333333335\n",
      "Loss tensor(0.0407, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 360\n",
      "episode random 0.0\n",
      "Avg random 3.7027027027027026\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 4.216216216216216\n",
      "episode rew 8.0\n",
      "Avg Rew -2.2972972972972974\n",
      "Loss tensor(0.0358, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 370\n",
      "episode random 3.0\n",
      "Avg random 3.6842105263157894\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 4.184210526315789\n",
      "episode rew 2.0\n",
      "Avg Rew -2.1842105263157894\n",
      "Loss tensor(0.0523, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 380\n",
      "episode random 1.0\n",
      "Avg random 3.6153846153846154\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 4.153846153846154\n",
      "episode rew 2.0\n",
      "Avg Rew -2.076923076923077\n",
      "Loss tensor(0.0382, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 390\n",
      "episode random 1.0\n",
      "Avg random 3.55\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 4.075\n",
      "episode rew 4.0\n",
      "Avg Rew -1.925\n",
      "Loss tensor(0.0465, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 400\n",
      "episode random 2.0\n",
      "Avg random 3.5121951219512195\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 3.975609756097561\n",
      "episode rew 8.0\n",
      "Avg Rew -1.6829268292682926\n",
      "Loss tensor(0.0428, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 410\n",
      "episode random 0.0\n",
      "Avg random 3.4285714285714284\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 3.880952380952381\n",
      "episode rew 8.0\n",
      "Avg Rew -1.4523809523809523\n",
      "Loss tensor(0.0197, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 420\n",
      "episode random 1.0\n",
      "Avg random 3.372093023255814\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 3.7906976744186047\n",
      "episode rew 6.0\n",
      "Avg Rew -1.2790697674418605\n",
      "Loss tensor(0.0411, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 430\n",
      "episode random 3.0\n",
      "Avg random 3.3636363636363638\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 3.772727272727273\n",
      "episode rew 0.0\n",
      "Avg Rew -1.25\n",
      "Loss tensor(0.0347, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "game 440\n",
      "episode random 2.0\n",
      "Avg random 3.3333333333333335\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 3.7111111111111112\n",
      "episode rew 4.0\n",
      "Avg Rew -1.1333333333333333\n",
      "Loss tensor(0.0405, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 450\n",
      "episode random 0.0\n",
      "Avg random 3.260869565217391\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 3.630434782608696\n",
      "episode rew 8.0\n",
      "Avg Rew -0.9347826086956522\n",
      "Loss tensor(0.0685, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 460\n",
      "episode random 0.0\n",
      "Avg random 3.1914893617021276\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 3.5531914893617023\n",
      "episode rew 8.0\n",
      "Avg Rew -0.7446808510638298\n",
      "Loss tensor(0.0503, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 470\n",
      "episode random 1.0\n",
      "Avg random 3.1458333333333335\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 3.4791666666666665\n",
      "episode rew 8.0\n",
      "Avg Rew -0.5625\n",
      "Loss tensor(0.0603, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 480\n",
      "episode random 1.0\n",
      "Avg random 3.1020408163265305\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 3.4081632653061225\n",
      "episode rew 8.0\n",
      "Avg Rew -0.3877551020408163\n",
      "Loss tensor(0.0307, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 490\n",
      "episode random 4.0\n",
      "Avg random 3.12\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 3.38\n",
      "episode rew 2.0\n",
      "Avg Rew -0.34\n",
      "Loss tensor(0.0441, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 500\n",
      "episode random 2.0\n",
      "Avg random 3.16\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 3.4\n",
      "episode rew 4.0\n",
      "Avg Rew -0.26\n",
      "Loss tensor(0.0243, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 510\n",
      "episode random 0.0\n",
      "Avg random 2.56\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 3.32\n",
      "episode rew 7.0\n",
      "Avg Rew 0.04\n",
      "Loss tensor(0.0310, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 520\n",
      "episode random 1.0\n",
      "Avg random 2.02\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 3.26\n",
      "episode rew 6.0\n",
      "Avg Rew 0.22\n",
      "Loss tensor(0.0450, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 530\n",
      "episode random 2.0\n",
      "Avg random 1.78\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 3.2\n",
      "episode rew 6.0\n",
      "Avg Rew 0.5\n",
      "Loss tensor(0.0233, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 540\n",
      "episode random 2.0\n",
      "Avg random 1.5\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 3.12\n",
      "episode rew 0.0\n",
      "Avg Rew 0.64\n",
      "Loss tensor(0.0395, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 550\n",
      "episode random 1.0\n",
      "Avg random 1.46\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 2.96\n",
      "episode rew 8.0\n",
      "Avg Rew 0.96\n",
      "Loss tensor(0.0349, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 560\n",
      "episode random 0.0\n",
      "Avg random 1.42\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 2.86\n",
      "episode rew 5.0\n",
      "Avg Rew 1.18\n",
      "Loss tensor(0.0384, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 570\n",
      "episode random 1.0\n",
      "Avg random 1.38\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 2.72\n",
      "episode rew 8.0\n",
      "Avg Rew 1.5\n",
      "Loss tensor(0.0329, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 580\n",
      "episode random 1.0\n",
      "Avg random 1.4\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 2.54\n",
      "episode rew 6.0\n",
      "Avg Rew 1.82\n",
      "Loss tensor(0.0396, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 590\n",
      "episode random 2.0\n",
      "Avg random 1.44\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 2.38\n",
      "episode rew 4.0\n",
      "Avg Rew 2.1\n",
      "Loss tensor(0.0389, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 600\n",
      "episode random 5.0\n",
      "Avg random 1.5\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 2.2\n",
      "episode rew -1.0\n",
      "Avg Rew 2.24\n",
      "Loss tensor(0.0521, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 610\n",
      "episode random 0.0\n",
      "Avg random 1.44\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 2.06\n",
      "episode rew 4.0\n",
      "Avg Rew 2.48\n",
      "Loss tensor(0.0410, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 620\n",
      "episode random 1.0\n",
      "Avg random 1.4\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 1.9\n",
      "episode rew 3.0\n",
      "Avg Rew 2.74\n",
      "Loss tensor(0.0574, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 630\n",
      "episode random 3.0\n",
      "Avg random 1.44\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 1.84\n",
      "episode rew 2.0\n",
      "Avg Rew 2.9\n",
      "Loss tensor(0.0500, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 640\n",
      "episode random 2.0\n",
      "Avg random 1.48\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.7\n",
      "episode rew 6.0\n",
      "Avg Rew 3.22\n",
      "Loss tensor(0.0498, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 650\n",
      "episode random 1.0\n",
      "Avg random 1.46\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.62\n",
      "episode rew 8.0\n",
      "Avg Rew 3.56\n",
      "Loss tensor(0.0387, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 660\n",
      "episode random 2.0\n",
      "Avg random 1.48\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 1.54\n",
      "episode rew 0.0\n",
      "Avg Rew 3.72\n",
      "Loss tensor(0.0412, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 670\n",
      "episode random 3.0\n",
      "Avg random 1.52\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 1.48\n",
      "episode rew 4.0\n",
      "Avg Rew 3.92\n",
      "Loss tensor(0.0593, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 680\n",
      "episode random 2.0\n",
      "Avg random 1.48\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.44\n",
      "episode rew 6.0\n",
      "Avg Rew 4.04\n",
      "Loss tensor(0.0752, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 690\n",
      "episode random 2.0\n",
      "Avg random 1.5\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.38\n",
      "episode rew 6.0\n",
      "Avg Rew 4.16\n",
      "Loss tensor(0.0347, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 700\n",
      "episode random 0.0\n",
      "Avg random 1.46\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 1.34\n",
      "episode rew 6.0\n",
      "Avg Rew 4.28\n",
      "Loss tensor(0.0491, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 710\n",
      "episode random 1.0\n",
      "Avg random 1.44\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 1.32\n",
      "episode rew 5.0\n",
      "Avg Rew 4.46\n",
      "Loss tensor(0.0494, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 720\n",
      "episode random 1.0\n",
      "Avg random 1.46\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.32\n",
      "episode rew 8.0\n",
      "Avg Rew 4.5\n",
      "Loss tensor(0.0728, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 730\n",
      "episode random 3.0\n",
      "Avg random 1.46\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.26\n",
      "episode rew 5.0\n",
      "Avg Rew 4.6\n",
      "Loss tensor(0.0524, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 740\n",
      "episode random 3.0\n",
      "Avg random 1.52\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.22\n",
      "episode rew 4.0\n",
      "Avg Rew 4.64\n",
      "Loss tensor(0.0424, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 750\n",
      "episode random 2.0\n",
      "Avg random 1.5\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.14\n",
      "episode rew 10.0\n",
      "Avg Rew 4.84\n",
      "Loss tensor(0.0410, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 760\n",
      "episode random 1.0\n",
      "Avg random 1.46\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.06\n",
      "episode rew 8.0\n",
      "Avg Rew 5.1\n",
      "Loss tensor(0.0390, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 770\n",
      "episode random 2.0\n",
      "Avg random 1.44\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.08\n",
      "episode rew 7.0\n",
      "Avg Rew 5.14\n",
      "Loss tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 780\n",
      "episode random 1.0\n",
      "Avg random 1.44\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 1.14\n",
      "episode rew 3.0\n",
      "Avg Rew 5.12\n",
      "Loss tensor(0.0400, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 790\n",
      "episode random 3.0\n",
      "Avg random 1.48\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.12\n",
      "episode rew 4.0\n",
      "Avg Rew 5.08\n",
      "Loss tensor(0.0522, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 800\n",
      "episode random 0.0\n",
      "Avg random 1.48\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.04\n",
      "episode rew 8.0\n",
      "Avg Rew 5.24\n",
      "Loss tensor(0.0271, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 810\n",
      "episode random 2.0\n",
      "Avg random 1.52\n",
      "episode bad predi 3.0\n",
      "Avg bad predi 1.08\n",
      "episode rew 2.0\n",
      "Avg Rew 5.16\n",
      "Loss tensor(0.0330, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 820\n",
      "episode random 2.0\n",
      "Avg random 1.46\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.06\n",
      "episode rew 7.0\n",
      "Avg Rew 5.22\n",
      "Loss tensor(0.0373, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 830\n",
      "episode random 1.0\n",
      "Avg random 1.48\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.06\n",
      "episode rew 7.0\n",
      "Avg Rew 5.24\n",
      "Loss tensor(0.0427, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 840\n",
      "episode random 2.0\n",
      "Avg random 1.52\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.02\n",
      "episode rew 6.0\n",
      "Avg Rew 5.28\n",
      "Loss tensor(0.0330, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 850\n",
      "episode random 5.0\n",
      "Avg random 1.62\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.0\n",
      "episode rew 7.0\n",
      "Avg Rew 5.3\n",
      "Loss tensor(0.0392, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 860\n",
      "episode random 1.0\n",
      "Avg random 1.64\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.0\n",
      "episode rew 5.0\n",
      "Avg Rew 5.24\n",
      "Loss tensor(0.0496, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 870\n",
      "episode random 1.0\n",
      "Avg random 1.6\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.96\n",
      "episode rew 4.0\n",
      "Avg Rew 5.28\n",
      "Loss tensor(0.0343, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 880\n",
      "episode random 4.0\n",
      "Avg random 1.66\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.9\n",
      "episode rew 7.0\n",
      "Avg Rew 5.38\n",
      "Loss tensor(0.0380, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 890\n",
      "episode random 0.0\n",
      "Avg random 1.64\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.88\n",
      "episode rew 6.0\n",
      "Avg Rew 5.42\n",
      "Loss tensor(0.0596, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 900\n",
      "episode random 0.0\n",
      "Avg random 1.6\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.9\n",
      "episode rew 6.0\n",
      "Avg Rew 5.38\n",
      "Loss tensor(0.0426, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 910\n",
      "episode random 1.0\n",
      "Avg random 1.62\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.9\n",
      "episode rew 7.0\n",
      "Avg Rew 5.36\n",
      "Loss tensor(0.0483, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 920\n",
      "episode random 1.0\n",
      "Avg random 1.62\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.9\n",
      "episode rew 4.0\n",
      "Avg Rew 5.32\n",
      "Loss tensor(0.0343, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 930\n",
      "episode random 4.0\n",
      "Avg random 1.64\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.84\n",
      "episode rew 9.0\n",
      "Avg Rew 5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(0.0621, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 940\n",
      "episode random 2.0\n",
      "Avg random 1.64\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.82\n",
      "episode rew 10.0\n",
      "Avg Rew 5.62\n",
      "Loss tensor(0.0441, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 950\n",
      "episode random 0.0\n",
      "Avg random 1.64\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.82\n",
      "episode rew 10.0\n",
      "Avg Rew 5.66\n",
      "Loss tensor(0.0390, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 960\n",
      "episode random 0.0\n",
      "Avg random 1.64\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.84\n",
      "episode rew 8.0\n",
      "Avg Rew 5.66\n",
      "Loss tensor(0.0365, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 970\n",
      "episode random 2.0\n",
      "Avg random 1.66\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.86\n",
      "episode rew 2.0\n",
      "Avg Rew 5.54\n",
      "Loss tensor(0.0328, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 980\n",
      "episode random 2.0\n",
      "Avg random 1.68\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.86\n",
      "episode rew 6.0\n",
      "Avg Rew 5.5\n",
      "Loss tensor(0.0559, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 990\n",
      "episode random 2.0\n",
      "Avg random 1.64\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.84\n",
      "episode rew 6.0\n",
      "Avg Rew 5.58\n",
      "Loss tensor(0.0519, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1000\n",
      "episode random 6.0\n",
      "Avg random 1.72\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.82\n",
      "episode rew 6.0\n",
      "Avg Rew 5.62\n",
      "Loss tensor(0.0215, grad_fn=<HuberLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Main Training Loop\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "player_opt = OptimalPlayer(epsilon=0.7, player='O')\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=5e-4)\n",
    "rew_buffer = deque([0.0], maxlen=50)\n",
    "bad_pred_buffer = deque([0.0], maxlen=50)\n",
    "random_buffer = deque([0.0], maxlen=50)\n",
    "\n",
    "game = 0\n",
    "old_game = 0\n",
    "step = 0\n",
    "bad_prediction = False\n",
    "\n",
    "episode_reward = 0.0\n",
    "episode_bad_predict = 0.0\n",
    "episode_random = 0.0\n",
    "\n",
    "criterion = torch.nn.HuberLoss(delta=1.0)\n",
    "\n",
    "while(game < 1000):\n",
    "    step +=1\n",
    "    epsilon = np.interp(step, [0, EPS_DECAY], [EPS_START,EPS_END])\n",
    "    \n",
    "    rnd_sample = random.random()\n",
    "    #print('rnd_sample', rnd_sample)\n",
    "    \n",
    "   \n",
    "    #random agent play\n",
    "    obs = env.observe()[0]\n",
    "    available_action = find_aval_actions(obs, list_of_action)\n",
    "    if rnd_sample <= epsilon:\n",
    "        random_int = random.randint(0,len(available_action)-1)\n",
    "        #print('random_int', random_int, 'len available action',len(available_action))\n",
    "        move = available_action[random_int]\n",
    "        #print('move',move)\n",
    "        episode_random +=1\n",
    "    else:\n",
    "        move = policy_net.act(obs)\n",
    "        #print('move',move)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not move in available_action:\n",
    "        #print(' bad prediction', available_action)\n",
    "        bad_prediction = True\n",
    "        \n",
    "    obs = env.observe()[0].flatten()\n",
    "    #print('move prediction', move)\n",
    "    #env.render()\n",
    "    #print('move', move)\n",
    "    \n",
    "    if not bad_prediction:\n",
    "        #print('env step')\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        #env.render()\n",
    "        \n",
    "    action = move\n",
    "    #print('###### action #####',action)\n",
    "    #env.render()\n",
    "    \n",
    "    if (not end) and (not bad_prediction):\n",
    "        #optimal player play\n",
    "        move = player_opt.act(grid)\n",
    "        #print('optimal move', move)\n",
    "        \n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        #env.render()\n",
    "    \n",
    "\n",
    "    #transition set-off\n",
    "    new_obs = env.observe()[0].flatten()\n",
    "    if not bad_prediction:\n",
    "        rew = env.reward(player='X')\n",
    "        transition = (obs, action, rew, end, new_obs)\n",
    "    else:\n",
    "        rew = -1\n",
    "        transition = (obs, action, rew, True, new_obs)\n",
    "        #end = False\n",
    "    \n",
    "    #print('transition', transition)\n",
    "    replay_buffer.append(transition)\n",
    "    \n",
    "    \n",
    "    if end or bad_prediction:\n",
    "        #print('game finish')\n",
    "        if bad_prediction:\n",
    "            episode_bad_predict += 1\n",
    "            \n",
    "        bad_prediction = False\n",
    "        #print('-------------------------------------------')\n",
    "        #print('Game end, winner is player ' + str(winner))\n",
    "        #print('Optimal player = ' +  Turns[0])\n",
    "        #print('Random player = ' +  Turns[1])\n",
    "        #env.render()\n",
    "        env.reset()\n",
    "        episode_reward += rew\n",
    "        game += 1\n",
    "        if game % 10 == 0:\n",
    "            \n",
    "            rew_buffer.append(episode_reward)\n",
    "            bad_pred_buffer.append(episode_bad_predict)\n",
    "            random_buffer.append(episode_random)\n",
    "\n",
    "            # Logging\n",
    "            if game % 10 == 0:\n",
    "                print()\n",
    "                print('game', game)\n",
    "                print('episode random',episode_random)\n",
    "                print('Avg random', np.mean(random_buffer))\n",
    "                print('episode bad predi',episode_bad_predict)\n",
    "                print('Avg bad predi', np.mean(bad_pred_buffer))\n",
    "                print('episode rew',episode_reward)\n",
    "                print('Avg Rew', np.mean(rew_buffer)) \n",
    "                print('Loss', loss)\n",
    "            episode_reward = 0.0\n",
    "            episode_bad_predict = 0.0\n",
    "            episode_random = 0.0\n",
    "    \n",
    "    \n",
    "    # Start Gradient Step\n",
    "    #print('replay_buffer', len(replay_buffer))\n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    \n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    #print('obsas', type(obses))\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    #print('actions type', type(actions))\n",
    "    #print(actions)\n",
    "    rews = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "    \n",
    "    obses_tensor = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    actions_tensor = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rews_tensor = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_tensor = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    #print('dones tensor', dones_tensor.shape)\n",
    "    new_obses_tensor = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_obses_tensor)\n",
    "    #print('target_q_values',target_q_values.shape)\n",
    "    #mask_possible_action = torch.ne(new_obses_tensor,0)*-1\n",
    "    #print('mask_possible_action',mask_possible_action.shape)\n",
    "    #possible_target_q_values = target_q_values * mask_possible_action\n",
    "    #print('possible_target_q_values',possible_target_q_values.shape)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    #print('max_target_q_values', max_target_q_values.shape)\n",
    "    \n",
    "    targets = rews_tensor + GAMMA * ( 1 - dones_tensor)  * max_target_q_values\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute Loss\n",
    "    \n",
    "    q_values = policy_net(obses_tensor)\n",
    "    #print('q_value',q_values.shape)\n",
    "    \n",
    "    #y_pred = q_values.gather(1, actions_tensor.long().view((BATCH_SIZE, 1))).view(-1)\n",
    "    \n",
    "    #q_values_max = q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    #print('targets', targets.shape, 'rews tensor', rews_tensor.shape)\n",
    "    #print('q_values',q_values.shape)\n",
    "    #print('action reshape', actions_tensor.long().view((BATCH_SIZE, 1)).shape)\n",
    "    #print('y pred', y_pred.shape)\n",
    "    #print('q_values_max',q_values_max.shape)\n",
    "    \n",
    "    \n",
    "    #print('q_values',q_values.shape,'q_values_max',q_values_max.shape)\n",
    "    #print('actions tensor',actions_tensor.shape)\n",
    "    \n",
    "    # give the q values of the action we took\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_tensor)\n",
    "    \n",
    "#     print('action_q_values',action_q_values.shape, 'targets',targets.shape)\n",
    "#     print('actions_tensor', actions_tensor[0], actions_tensor[1])\n",
    "#     print('q_values', q_values[0])\n",
    "#     print('q_values row 1', q_values[1])\n",
    "#     print('action_q_values',action_q_values[0])\n",
    "    \n",
    "    #loss = nn.functional.smooth_l1_loss(q_values_max, targets)\n",
    "    loss = criterion(action_q_values, targets)\n",
    "   \n",
    "    \n",
    "    #print('loss', loss, loss.shape, type(loss))\n",
    "#     max_q_idx = torch.argmax(possible_q_values, dim=1)[0]\n",
    "#     max_q_idx_item = max_q_idx.detach().item()\n",
    "    \n",
    "    \n",
    "    # Gradient Descent\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    # Logging\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
