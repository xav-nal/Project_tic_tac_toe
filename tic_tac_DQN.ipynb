{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "list_of_action = {\n",
    "    0: (0,0),\n",
    "    1: (0,1),\n",
    "    2: (0,2),\n",
    "    3: (1,0),\n",
    "    4: (1,1),\n",
    "    5: (1,2),\n",
    "    6: (2,0),\n",
    "    7: (2,1),\n",
    "    8: (2,2)\n",
    "}\n",
    "\n",
    "rev_action = {\n",
    "    (0,0) :0,\n",
    "    (0,1) :1,\n",
    "    (0,2) :2,\n",
    "    (1,0) :3,\n",
    "    (1,1) :4,\n",
    "    (1,2) :5,\n",
    "    (2,0) :6,\n",
    "    (2,1) :7,\n",
    "    (2,2) :8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_aval_actions(state_of_game,act):\n",
    "  possible_actions = []\n",
    "  for i in range(9):\n",
    "    if state_of_game[act[i]] == 0:\n",
    "      possible_actions.append(i)\n",
    "  return possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, outputs, action_list):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.action_list = action_list\n",
    "        #print('input size',in_features )\n",
    "        \n",
    "        \n",
    "        self.head1 = nn.Linear(in_features, 128)\n",
    "        self.head2 = nn.Linear(128,128)\n",
    "        self.outputlayer = nn.Linear(128,outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        #print('input x', x.shape, type(x))\n",
    "        #print('x reshape',x.reshape(x.shape(0),-1))\n",
    "        Batch_size,_,_,_ = x.shape\n",
    "        \n",
    "        #print('x reshape',x.reshape(Batch_size,-1).shape)\n",
    "        \n",
    "        x = F.relu(self.head1(torch.Tensor(x).reshape(Batch_size,-1)))\n",
    "        x = F.relu(self.head2(x))\n",
    "        x = self.outputlayer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def act(self, state):\n",
    "        state_tensor = torch.as_tensor(state, dtype=torch.float32)\n",
    "        #print('----------------------------------------------------------')\n",
    "        #print('act', state_tensor.shape, state_tensor.unsqueeze(-1).shape )\n",
    "        q_values = self.forward(state_tensor.unsqueeze(0))\n",
    "        #possible_q_values = mask_possible_action*q_values\n",
    "        #print('possible_q_values',possible_q_values)\n",
    "        \n",
    "        \n",
    "        max_q_idx = torch.argmax(q_values, dim=1)[0]\n",
    "        max_q_idx_item = max_q_idx.detach().item()\n",
    "        #print('max_q_idx_item', max_q_idx_item, type(max_q_idx_item))\n",
    "        #print('action list', self.action_list)\n",
    "        #print('rev_action',rev_action)\n",
    "        move = self.action_list[max_q_idx_item]\n",
    "        \n",
    "        action_idx = rev_action[move]\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE_FREQ = 500\n",
    "\n",
    "number_of_actions = 9\n",
    "state_features = 18\n",
    "\n",
    "\n",
    "env = TictactoeEnv()\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "rew_buffer = deque([0.0], maxlen=10)\n",
    "\n",
    "episode_reward = 0.0\n",
    "\n",
    "#initialization network\n",
    "policy_net = DQN(state_features, number_of_actions,list_of_action).to(\"cpu\")\n",
    "target_net = DQN(state_features, number_of_actions,list_of_action).to(\"cpu\")\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "#target_net.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_state(grid, agent_tag):\n",
    "    #print('Player',agent_tag,'grid',grid)\n",
    "    #print('grid', type(grid))\n",
    "    grid_tensor = torch.as_tensor(grid, dtype=torch.float32)\n",
    "    x_state = torch.zeros((3,3,2))\n",
    "    if agent_tag == 'X':\n",
    "        x_state[:,:,0] = torch.eq(grid_tensor,1).int()\n",
    "        x_state[:,:,1] = torch.eq(grid_tensor,-1).int()\n",
    "    else:\n",
    "        x_state[:,:,0] = torch.eq(grid_tensor,-1).int()\n",
    "        x_state[:,:,1] = torch.eq(grid_tensor,1).int()\n",
    "    \n",
    "    #print('agent', x_state[:,:,0])\n",
    "    #print('opponent',x_state[:,:,1])\n",
    "    x_state_array = x_state.numpy()\n",
    "    return x_state_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games 0 buffer 1\n",
      "games 1 buffer 2\n",
      "games 2 buffer 3\n",
      "games 3 buffer 4\n",
      "games 4 buffer 5\n",
      "games 5 buffer 6\n",
      "games 6 buffer 7\n",
      "games 7 buffer 8\n",
      "games 8 buffer 9\n",
      "games 9 buffer 10\n",
      "games 10 buffer 11\n",
      "games 11 buffer 12\n",
      "games 12 buffer 13\n",
      "games 13 buffer 14\n",
      "games 14 buffer 15\n",
      "games 15 buffer 16\n",
      "games 16 buffer 17\n",
      "games 17 buffer 18\n",
      "games 18 buffer 19\n",
      "games 19 buffer 20\n",
      "games 20 buffer 21\n",
      "games 21 buffer 22\n",
      "games 22 buffer 23\n",
      "games 23 buffer 24\n",
      "games 24 buffer 25\n",
      "games 25 buffer 26\n",
      "games 26 buffer 27\n",
      "games 27 buffer 28\n",
      "games 28 buffer 29\n",
      "games 29 buffer 30\n",
      "games 30 buffer 31\n",
      "games 31 buffer 32\n",
      "games 32 buffer 33\n",
      "games 33 buffer 34\n",
      "games 34 buffer 35\n",
      "games 35 buffer 36\n",
      "games 36 buffer 37\n",
      "games 37 buffer 38\n",
      "games 38 buffer 39\n",
      "games 39 buffer 40\n",
      "games 40 buffer 41\n",
      "games 41 buffer 42\n",
      "games 42 buffer 43\n",
      "games 43 buffer 44\n",
      "games 44 buffer 45\n",
      "games 45 buffer 46\n",
      "games 46 buffer 47\n",
      "games 47 buffer 48\n",
      "games 48 buffer 49\n",
      "games 49 buffer 50\n",
      "games 50 buffer 51\n",
      "games 51 buffer 52\n",
      "games 52 buffer 53\n",
      "games 53 buffer 54\n",
      "games 54 buffer 55\n",
      "games 55 buffer 56\n",
      "games 56 buffer 57\n",
      "games 57 buffer 58\n",
      "games 58 buffer 59\n",
      "games 59 buffer 60\n",
      "games 60 buffer 61\n",
      "games 61 buffer 62\n",
      "games 62 buffer 63\n",
      "games 63 buffer 64\n",
      "games 64 buffer 65\n",
      "games 65 buffer 66\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "MIN_REPLAY_SIZE = 10000\n",
    "\n",
    "#Initialize the Replay Buffer\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "games = 0\n",
    "episode_reward = 0.0\n",
    "\n",
    "while(len(replay_buffer) < BATCH_SIZE):\n",
    "\n",
    "    env.reset()\n",
    "    #env.render()\n",
    "    player1 = \"X\"\n",
    "    player2 = \"O\"\n",
    "    \n",
    "    if games%2 == 0:\n",
    "        player_opt = OptimalPlayer(epsilon = 1., player = player2)\n",
    "        player_rnd = OptimalPlayer(epsilon = 1., player = player1)\n",
    "        \n",
    "    if games%2 == 1:\n",
    "\n",
    "        #change the state by *-1 when optimal player is X\n",
    "        player_opt = OptimalPlayer(epsilon = 0.5, player = player1)\n",
    "        player_rnd = OptimalPlayer(epsilon = 1., player = player2)\n",
    "        grid, _, __ = env.observe()\n",
    "        comp_move = player_opt.act(grid)\n",
    "        env.step(comp_move)\n",
    "    \n",
    "\n",
    "    \n",
    "    for j in range(5):\n",
    "\n",
    "       \n",
    "        #random agent play\n",
    "        obs = env.observe()[0]\n",
    "        state = setup_state(obs,player_rnd.player)\n",
    "        move = player_rnd.act(obs)\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        action = rev_action[move]\n",
    "        #print('action', action\n",
    "        \n",
    "        \n",
    "        if not end:\n",
    "            #optimal player play\n",
    "            move = player_opt.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        \n",
    "        new_obs = env.observe()[0]\n",
    "        new_state = setup_state(new_obs,player_rnd.player)\n",
    "       \n",
    "    \n",
    "        #transition set-off\n",
    "        rew = env.reward(player=Turns[1])\n",
    "        transition = (state, action, rew, end, new_state)\n",
    "        #print('transition', transition)\n",
    "        replay_buffer.append(transition)\n",
    "           \n",
    "        print('games', games, 'buffer', len(replay_buffer))\n",
    "        games += 1\n",
    "        \n",
    "        if end:\n",
    "            #print('-------------------------------------------')\n",
    "            #print('Game end, winner is player ' + str(winner))\n",
    "            #print('Optimal player = ' +  Turns[0])\n",
    "            #print('Random player = ' +  Turns[1])\n",
    "            #env.render()\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for _ in range(MIN_REPLAY_SIZE):\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(replay_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]], dtype=float32), 5, 0, False, array([[[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 1.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 0.]],\n",
      "\n",
      "       [[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(replay_buffer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - -|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n",
      "\n",
      "game 100\n",
      "episode random 1.0\n",
      "Avg random 9.454545454545455\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 2.0\n",
      "episode rew 1.0\n",
      "Avg Rew -2.090909090909091\n",
      "Loss tensor(0.0591, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 200\n",
      "episode random 4.0\n",
      "Avg random 5.761904761904762\n",
      "episode bad predi 2.0\n",
      "Avg bad predi 1.5714285714285714\n",
      "episode rew 2.0\n",
      "Avg Rew -2.2857142857142856\n",
      "Loss tensor(0.0508, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 300\n",
      "episode random 3.0\n",
      "Avg random 4.548387096774194\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.5161290322580645\n",
      "episode rew -4.0\n",
      "Avg Rew -2.3548387096774195\n",
      "Loss tensor(0.0700, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 400\n",
      "episode random 2.0\n",
      "Avg random 3.975609756097561\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.4878048780487805\n",
      "episode rew 5.0\n",
      "Avg Rew -1.6341463414634145\n",
      "Loss tensor(0.0691, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 500\n",
      "episode random 0.0\n",
      "Avg random 3.450980392156863\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.3529411764705883\n",
      "episode rew 3.0\n",
      "Avg Rew -1.1372549019607843\n",
      "Loss tensor(0.0640, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 600\n",
      "episode random 1.0\n",
      "Avg random 3.098360655737705\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.2622950819672132\n",
      "episode rew 6.0\n",
      "Avg Rew -0.6065573770491803\n",
      "Loss tensor(0.0603, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 700\n",
      "episode random 2.0\n",
      "Avg random 3.0704225352112675\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.2112676056338028\n",
      "episode rew 9.0\n",
      "Avg Rew -0.056338028169014086\n",
      "Loss tensor(0.0664, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 800\n",
      "episode random 2.0\n",
      "Avg random 2.9753086419753085\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 1.123456790123457\n",
      "episode rew 4.0\n",
      "Avg Rew 0.5925925925925926\n",
      "Loss tensor(0.0601, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 900\n",
      "episode random 3.0\n",
      "Avg random 2.912087912087912\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 1.010989010989011\n",
      "episode rew 2.0\n",
      "Avg Rew 1.1978021978021978\n",
      "Loss tensor(0.0617, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1000\n",
      "episode random 5.0\n",
      "Avg random 2.9\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.94\n",
      "episode rew 2.0\n",
      "Avg Rew 1.63\n",
      "Loss tensor(0.0724, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1100\n",
      "episode random 1.0\n",
      "Avg random 2.12\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.74\n",
      "episode rew 9.0\n",
      "Avg Rew 2.48\n",
      "Loss tensor(0.0657, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1200\n",
      "episode random 1.0\n",
      "Avg random 2.24\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.67\n",
      "episode rew 2.0\n",
      "Avg Rew 3.29\n",
      "Loss tensor(0.0536, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1300\n",
      "episode random 3.0\n",
      "Avg random 2.29\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.54\n",
      "episode rew 6.0\n",
      "Avg Rew 4.13\n",
      "Loss tensor(0.0553, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1400\n",
      "episode random 3.0\n",
      "Avg random 2.25\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.42\n",
      "episode rew 4.0\n",
      "Avg Rew 4.69\n",
      "Loss tensor(0.0471, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1500\n",
      "episode random 1.0\n",
      "Avg random 2.29\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.34\n",
      "episode rew 8.0\n",
      "Avg Rew 5.29\n",
      "Loss tensor(0.0296, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1600\n",
      "episode random 0.0\n",
      "Avg random 2.33\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.27\n",
      "episode rew 8.0\n",
      "Avg Rew 5.84\n",
      "Loss tensor(0.0489, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1700\n",
      "episode random 3.0\n",
      "Avg random 2.23\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.2\n",
      "episode rew 6.0\n",
      "Avg Rew 6.05\n",
      "Loss tensor(0.0375, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1800\n",
      "episode random 3.0\n",
      "Avg random 2.18\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.16\n",
      "episode rew 7.0\n",
      "Avg Rew 6.14\n",
      "Loss tensor(0.0434, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 1900\n",
      "episode random 1.0\n",
      "Avg random 2.14\n",
      "episode bad predi 1.0\n",
      "Avg bad predi 0.16\n",
      "episode rew 8.0\n",
      "Avg Rew 6.29\n",
      "Loss tensor(0.0462, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 2000\n",
      "episode random 0.0\n",
      "Avg random 1.99\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.14\n",
      "episode rew 8.0\n",
      "Avg Rew 6.43\n",
      "Loss tensor(0.0486, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 2100\n",
      "episode random 0.0\n",
      "Avg random 1.89\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.16\n",
      "episode rew 8.0\n",
      "Avg Rew 6.49\n",
      "Loss tensor(0.0497, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 2200\n",
      "episode random 0.0\n",
      "Avg random 1.78\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.13\n",
      "episode rew 7.0\n",
      "Avg Rew 6.55\n",
      "Loss tensor(0.0405, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 2300\n",
      "episode random 1.0\n",
      "Avg random 1.7\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.14\n",
      "episode rew 6.0\n",
      "Avg Rew 6.6\n",
      "Loss tensor(0.0498, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 2400\n",
      "episode random 2.0\n",
      "Avg random 1.73\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.12\n",
      "episode rew 8.0\n",
      "Avg Rew 6.66\n",
      "Loss tensor(0.0304, grad_fn=<HuberLossBackward0>)\n",
      "\n",
      "game 2500\n",
      "episode random 1.0\n",
      "Avg random 1.83\n",
      "episode bad predi 0.0\n",
      "Avg bad predi 0.12\n",
      "episode rew 7.0\n",
      "Avg Rew 6.72\n",
      "Loss tensor(0.0373, grad_fn=<HuberLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Main Training Loop\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "player_opt = OptimalPlayer(epsilon=0.5, player='O')\n",
    "agent_tag = 'X'\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=5e-4)\n",
    "rew_buffer = deque([0.0], maxlen=100)\n",
    "bad_pred_buffer = deque([0.0], maxlen=100)\n",
    "random_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "game = 0\n",
    "old_game = 0\n",
    "step = 0\n",
    "bad_prediction = False\n",
    "\n",
    "episode_reward = 0.0\n",
    "episode_bad_predict = 0.0\n",
    "episode_random = 0.0\n",
    "\n",
    "criterion = torch.nn.HuberLoss(delta=1.0)\n",
    "\n",
    "while(game < 2500):\n",
    "    step +=1\n",
    "    epsilon = np.interp(step, [0, EPS_DECAY], [EPS_START,EPS_END])\n",
    "    \n",
    "    rnd_sample = random.random()\n",
    "    #print('rnd_sample', rnd_sample)\n",
    "    \n",
    "   \n",
    "    #random agent play\n",
    "    obs = env.observe()[0]\n",
    "    state = setup_state(obs,agent_tag)\n",
    "    available_action = find_aval_actions(obs, list_of_action)\n",
    "    if rnd_sample <= epsilon:\n",
    "        random_int = random.randint(0,len(available_action)-1)\n",
    "        #print('random_int', random_int, 'len available action',len(available_action))\n",
    "        move = available_action[random_int]\n",
    "        #print('move',move)\n",
    "        episode_random +=1\n",
    "    else:\n",
    "        move = policy_net.act(state)\n",
    "        #print('move',move)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not move in available_action:\n",
    "        #print(' bad prediction', available_action)\n",
    "        bad_prediction = True\n",
    "        \n",
    "    \n",
    "    \n",
    "    if not bad_prediction:\n",
    "        #print('env step')\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        #env.render()\n",
    "        \n",
    "    action = move\n",
    "    #print('###### action #####',action)\n",
    "    #env.render()\n",
    "    \n",
    "    if (not end) and (not bad_prediction):\n",
    "        #optimal player play\n",
    "        move = player_opt.act(grid)\n",
    "        #print('optimal move', move)\n",
    "        \n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        #env.render()\n",
    "    \n",
    "\n",
    "    #transition set-off\n",
    "    new_obs = env.observe()[0]\n",
    "    new_state = setup_state(new_obs,agent_tag)\n",
    "    if not bad_prediction:\n",
    "        rew = env.reward(player='X')\n",
    "        transition = (state, action, rew, end, new_state)\n",
    "    else:\n",
    "        rew = -1\n",
    "        transition = (state, action, rew, True, new_state)\n",
    "        #end = False\n",
    "    \n",
    "    #print('transition', transition)\n",
    "    replay_buffer.append(transition)\n",
    "    \n",
    "    \n",
    "    if end or bad_prediction:\n",
    "        #print('game finish')\n",
    "        if bad_prediction:\n",
    "            episode_bad_predict += 1\n",
    "            \n",
    "        bad_prediction = False\n",
    "        #print('-------------------------------------------')\n",
    "        #print('Game end, winner is player ' + str(winner))\n",
    "        #print('Optimal player = ' +  Turns[0])\n",
    "        #print('Random player = ' +  Turns[1])\n",
    "        #env.render()\n",
    "        env.reset()\n",
    "        episode_reward += rew\n",
    "        game += 1\n",
    "        if game % 10 == 0:\n",
    "            \n",
    "            rew_buffer.append(episode_reward)\n",
    "            bad_pred_buffer.append(episode_bad_predict)\n",
    "            random_buffer.append(episode_random)\n",
    "\n",
    "            # Logging\n",
    "            if game % 100 == 0:\n",
    "                print()\n",
    "                print('game', game)\n",
    "                print('episode random',episode_random)\n",
    "                print('Avg random', np.mean(random_buffer))\n",
    "                print('episode bad predi',episode_bad_predict)\n",
    "                print('Avg bad predi', np.mean(bad_pred_buffer))\n",
    "                print('episode rew',episode_reward)\n",
    "                print('Avg Rew', np.mean(rew_buffer)) \n",
    "                print('Loss', loss)\n",
    "            episode_reward = 0.0\n",
    "            episode_bad_predict = 0.0\n",
    "            episode_random = 0.0\n",
    "    \n",
    "    \n",
    "    # Start Gradient Step\n",
    "    #print('replay_buffer', len(replay_buffer))\n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    \n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    #print('obses', type(obses))\n",
    "    #print(obses)\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    #print('actions type', type(actions))\n",
    "    #print(actions)\n",
    "    rews = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "    \n",
    "    obses_tensor = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    actions_tensor = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rews_tensor = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_tensor = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_tensor = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "    \n",
    "    #print('obses_tensor', obses_tensor.shape)\n",
    "    #print('new_obses_tensor', new_obses_tensor.shape)\n",
    "    \n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_obses_tensor)\n",
    "    #print('target_q_values',target_q_values.shape)\n",
    "    #mask_possible_action = torch.ne(new_obses_tensor,0)*-1\n",
    "    #print('mask_possible_action',mask_possible_action.shape)\n",
    "    #possible_target_q_values = target_q_values * mask_possible_action\n",
    "    #print('possible_target_q_values',possible_target_q_values.shape)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    #print('max_target_q_values', max_target_q_values.shape)\n",
    "    \n",
    "    targets = rews_tensor + GAMMA * ( 1 - dones_tensor)  * max_target_q_values\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute Loss\n",
    "    \n",
    "    q_values = policy_net(obses_tensor)\n",
    "    \n",
    "    \n",
    "    # give the q values of the action we took\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_tensor)\n",
    "    \n",
    "#     print('action_q_values',action_q_values.shape, 'targets',targets.shape)\n",
    "#     print('actions_tensor', actions_tensor[0], actions_tensor[1])\n",
    "#     print('q_values', q_values[0])\n",
    "#     print('q_values row 1', q_values[1])\n",
    "#     print('action_q_values',action_q_values[0])\n",
    "    \n",
    "    #loss = nn.functional.smooth_l1_loss(q_values_max, targets)\n",
    "    loss = criterion(action_q_values, targets)\n",
    "   \n",
    "    \n",
    "    #print('loss', loss, loss.shape, type(loss))\n",
    "#     max_q_idx = torch.argmax(possible_q_values, dim=1)[0]\n",
    "#     max_q_idx_item = max_q_idx.detach().item()\n",
    "    \n",
    "    \n",
    "    # Gradient Descent\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    # Logging\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0. -1.]]\n"
     ]
    }
   ],
   "source": [
    "a[1,1] = 1\n",
    "a[2,2] = -1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_state = setup_state(a,'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [1. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "print(a_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 2)\n"
     ]
    }
   ],
   "source": [
    "a_state=a_state.reshape(9,2)\n",
    "print(a_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [1. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 1.]]]\n",
      "(3, 3, 2)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(a_state)\n",
    "print(a_state.shape)\n",
    "print(torch.Tensor(a_state).view(18))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(a_state.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True  True False]] (3, 3)\n"
     ]
    }
   ],
   "source": [
    "b = (a_state[:,:,0]==a_state[:,:,1])\n",
    "print(b,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "b = b.astype(int)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
