{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_action = {\n",
    "    0: (0,0),\n",
    "    1: (0,1),\n",
    "    2: (0,2),\n",
    "    3: (1,0),\n",
    "    4: (1,1),\n",
    "    5: (1,2),\n",
    "    6: (2,0),\n",
    "    7: (2,1),\n",
    "    8: (2,2)\n",
    "}\n",
    "\n",
    "rev_action = {\n",
    "    (0,0) :0,\n",
    "    (0,1) :1,\n",
    "    (0,2) :2,\n",
    "    (1,0) :3,\n",
    "    (1,1) :4,\n",
    "    (1,2) :5,\n",
    "    (2,0) :6,\n",
    "    (2,1) :7,\n",
    "    (2,2) :8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, outputs, action_list):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.action_list = action_list\n",
    "     \n",
    "        self.head1 = nn.Linear(in_features, 128)\n",
    "        self.head2 = nn.Linear(128,outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        #print('input x', x.shape, type(x))\n",
    "        x = F.relu(self.head1(x.view(x.size(0), -1)))\n",
    "        x = torch.sigmoid(self.head2(x))\n",
    "        #print('output x',x.shape)\n",
    "        return x\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        #mask_possible_action = torch.ne(obs_tensor,0)*-1\n",
    "        #print('mask_possible_action',mask_possible_action)\n",
    "        \n",
    "        q_values = self.forward(obs_tensor.unsqueeze(0))\n",
    "        #possible_q_values = mask_possible_action*q_values\n",
    "        #print('possible_q_values',possible_q_values)\n",
    "        \n",
    "        \n",
    "        max_q_idx = torch.argmax(q_values, dim=1)[0]\n",
    "        max_q_idx_item = max_q_idx.detach().item()\n",
    "        #print('max_q_idx_item', max_q_idx_item, type(max_q_idx_item))\n",
    "        #print('action list', self.action_list)\n",
    "        #print('rev_action',rev_action)\n",
    "        move = self.action_list[max_q_idx_item]\n",
    "        \n",
    "        action_idx = rev_action[move]\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "\n",
    "number_of_actions = 9\n",
    "grid_size = 9\n",
    "\n",
    "\n",
    "env = TictactoeEnv()\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "rew_buffer = deque([0.0], maxlen=10)\n",
    "\n",
    "episode_reward = 0.0\n",
    "\n",
    "#initialization network\n",
    "policy_net = DQN(grid_size, number_of_actions,list_of_action).to(\"cpu\")\n",
    "target_net = DQN(grid_size, number_of_actions,list_of_action).to(\"cpu\")\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "#target_net.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 1 buffer 0\n",
      "game 2 buffer 5\n",
      "game 3 buffer 9\n",
      "game 4 buffer 13\n",
      "game 5 buffer 16\n",
      "game 6 buffer 21\n",
      "game 7 buffer 26\n",
      "game 8 buffer 30\n",
      "game 9 buffer 34\n",
      "game 10 buffer 39\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "MIN_REPLAY_SIZE = 40\n",
    "\n",
    "#Initialize the Replay Buffer\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "game = 0\n",
    "episode_reward = 0.0\n",
    "\n",
    "while(len(replay_buffer) < MIN_REPLAY_SIZE):\n",
    "    game += 1\n",
    "    print('game', game, 'buffer', len(replay_buffer))\n",
    "    env.reset()\n",
    "    #env.render()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0.7, player='O')\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player='X')\n",
    "    \n",
    "    for j in range(5):\n",
    "        #print('step', j)\n",
    "       \n",
    "        #random agent play\n",
    "        obs = env.observe()[0].flatten()\n",
    "        move = player_rnd.act(grid)\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        action = rev_action[move]\n",
    "        \n",
    "        if not end:\n",
    "            #optimal player play\n",
    "            move = player_opt.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "        \n",
    "        new_obs = env.observe()[0].flatten()\n",
    "       \n",
    "    \n",
    "        #transition set-off\n",
    "        rew = env.reward(player=Turns[1])\n",
    "        transition = (obs, action, rew, end, new_obs)\n",
    "        replay_buffer.append(transition)\n",
    "           \n",
    "\n",
    "        if end:\n",
    "            #print('-------------------------------------------')\n",
    "            #print('Game end, winner is player ' + str(winner))\n",
    "            #print('Optimal player = ' +  Turns[0])\n",
    "            #print('Random player = ' +  Turns[1])\n",
    "            #env.render()\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for _ in range(MIN_REPLAY_SIZE):\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(replay_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.deque'>\n"
     ]
    }
   ],
   "source": [
    "print(type(replay_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.]), 1, 0, False, array([ 1.,  1., -1.,  0.,  0.,  0., -1.,  0.,  0.]))\n"
     ]
    }
   ],
   "source": [
    "print(replay_buffer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - -|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n",
      "\n",
      "game 10\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 4.5\n",
      "episode rew -10.0\n",
      "Avg Rew -5.0\n",
      "Loss tensor(0.3144, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 20\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 5.666666666666667\n",
      "episode rew -6.0\n",
      "Avg Rew -5.333333333333333\n",
      "Loss tensor(0.3901, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 30\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 6.5\n",
      "episode rew -10.0\n",
      "Avg Rew -6.5\n",
      "Loss tensor(0.3281, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 40\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 6.8\n",
      "episode rew -10.0\n",
      "Avg Rew -7.2\n",
      "Loss tensor(0.3219, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 50\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 7.166666666666667\n",
      "episode rew -10.0\n",
      "Avg Rew -7.666666666666667\n",
      "Loss tensor(0.3287, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 60\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 7.428571428571429\n",
      "episode rew -10.0\n",
      "Avg Rew -8.0\n",
      "Loss tensor(0.1881, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 70\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 7.5\n",
      "episode rew -8.0\n",
      "Avg Rew -8.0\n",
      "Loss tensor(0.2326, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 80\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 7.555555555555555\n",
      "episode rew -10.0\n",
      "Avg Rew -8.222222222222221\n",
      "Loss tensor(0.1617, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 90\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 7.8\n",
      "episode rew -10.0\n",
      "Avg Rew -8.4\n",
      "Loss tensor(0.2097, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 100\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 7.818181818181818\n",
      "episode rew -8.0\n",
      "Avg Rew -8.363636363636363\n",
      "Loss tensor(0.2207, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 110\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 7.833333333333333\n",
      "episode rew -10.0\n",
      "Avg Rew -8.5\n",
      "Loss tensor(0.1394, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 120\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 7.923076923076923\n",
      "episode rew -8.0\n",
      "Avg Rew -8.461538461538462\n",
      "Loss tensor(0.1914, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 130\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.0\n",
      "episode rew -10.0\n",
      "Avg Rew -8.571428571428571\n",
      "Loss tensor(0.2215, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 140\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 8.0\n",
      "episode rew -10.0\n",
      "Avg Rew -8.666666666666666\n",
      "Loss tensor(0.2204, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 150\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.0625\n",
      "episode rew -10.0\n",
      "Avg Rew -8.75\n",
      "Loss tensor(0.1655, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 160\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.117647058823529\n",
      "episode rew -10.0\n",
      "Avg Rew -8.823529411764707\n",
      "Loss tensor(0.1657, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 170\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.222222222222221\n",
      "episode rew -10.0\n",
      "Avg Rew -8.88888888888889\n",
      "Loss tensor(0.1989, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 180\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.263157894736842\n",
      "episode rew -10.0\n",
      "Avg Rew -8.947368421052632\n",
      "Loss tensor(0.2128, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 190\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.35\n",
      "episode rew -10.0\n",
      "Avg Rew -9.0\n",
      "Loss tensor(0.1295, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 200\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 8.333333333333334\n",
      "episode rew -8.0\n",
      "Avg Rew -8.952380952380953\n",
      "Loss tensor(0.0959, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 210\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.409090909090908\n",
      "episode rew -10.0\n",
      "Avg Rew -9.0\n",
      "Loss tensor(0.1600, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 220\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.347826086956522\n",
      "episode rew -8.0\n",
      "Avg Rew -8.956521739130435\n",
      "Loss tensor(0.1914, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 230\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 8.333333333333334\n",
      "episode rew -6.0\n",
      "Avg Rew -8.833333333333334\n",
      "Loss tensor(0.1594, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 240\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.28\n",
      "episode rew -10.0\n",
      "Avg Rew -8.88\n",
      "Loss tensor(0.1439, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 250\n",
      "episode bad predi 6.0\n",
      "Avg bad predi 8.192307692307692\n",
      "episode rew -6.0\n",
      "Avg Rew -8.76923076923077\n",
      "Loss tensor(0.1117, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 260\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.222222222222221\n",
      "episode rew -10.0\n",
      "Avg Rew -8.814814814814815\n",
      "Loss tensor(0.1912, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 270\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.25\n",
      "episode rew -10.0\n",
      "Avg Rew -8.857142857142858\n",
      "Loss tensor(0.2067, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 280\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.310344827586206\n",
      "episode rew -10.0\n",
      "Avg Rew -8.89655172413793\n",
      "Loss tensor(0.2068, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 290\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.266666666666667\n",
      "episode rew -4.0\n",
      "Avg Rew -8.733333333333333\n",
      "Loss tensor(0.2213, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 300\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.225806451612904\n",
      "episode rew -4.0\n",
      "Avg Rew -8.580645161290322\n",
      "Loss tensor(0.1266, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 310\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.1875\n",
      "episode rew -8.0\n",
      "Avg Rew -8.5625\n",
      "Loss tensor(0.1737, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 320\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 8.181818181818182\n",
      "episode rew -10.0\n",
      "Avg Rew -8.606060606060606\n",
      "Loss tensor(0.1576, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 330\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.147058823529411\n",
      "episode rew -8.0\n",
      "Avg Rew -8.588235294117647\n",
      "Loss tensor(0.1733, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 340\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.114285714285714\n",
      "episode rew -8.0\n",
      "Avg Rew -8.571428571428571\n",
      "Loss tensor(0.1886, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 350\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.083333333333334\n",
      "episode rew -8.0\n",
      "Avg Rew -8.555555555555555\n",
      "Loss tensor(0.1261, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 360\n",
      "episode bad predi 5.0\n",
      "Avg bad predi 8.0\n",
      "episode rew -4.0\n",
      "Avg Rew -8.432432432432432\n",
      "Loss tensor(0.1883, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 370\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 7.973684210526316\n",
      "episode rew -8.0\n",
      "Avg Rew -8.421052631578947\n",
      "Loss tensor(0.1890, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 380\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.025641025641026\n",
      "episode rew -10.0\n",
      "Avg Rew -8.461538461538462\n",
      "Loss tensor(0.1570, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 390\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.075\n",
      "episode rew -10.0\n",
      "Avg Rew -8.5\n",
      "Loss tensor(0.1890, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 400\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.121951219512194\n",
      "episode rew -10.0\n",
      "Avg Rew -8.536585365853659\n",
      "Loss tensor(0.1893, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 410\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.095238095238095\n",
      "episode rew -6.0\n",
      "Avg Rew -8.476190476190476\n",
      "Loss tensor(0.2204, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 420\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.13953488372093\n",
      "episode rew -10.0\n",
      "Avg Rew -8.511627906976743\n",
      "Loss tensor(0.1262, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 430\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.181818181818182\n",
      "episode rew -10.0\n",
      "Avg Rew -8.545454545454545\n",
      "Loss tensor(0.1888, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 440\n",
      "episode bad predi 7.0\n",
      "Avg bad predi 8.155555555555555\n",
      "episode rew -6.0\n",
      "Avg Rew -8.488888888888889\n",
      "Loss tensor(0.1887, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 450\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 8.152173913043478\n",
      "episode rew -8.0\n",
      "Avg Rew -8.478260869565217\n",
      "Loss tensor(0.2042, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 460\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.170212765957446\n",
      "episode rew -10.0\n",
      "Avg Rew -8.51063829787234\n",
      "Loss tensor(0.2197, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 470\n",
      "episode bad predi 9.0\n",
      "Avg bad predi 8.1875\n",
      "episode rew -8.0\n",
      "Avg Rew -8.5\n",
      "Loss tensor(0.2197, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 480\n",
      "episode bad predi 10.0\n",
      "Avg bad predi 8.224489795918368\n",
      "episode rew -10.0\n",
      "Avg Rew -8.53061224489796\n",
      "Loss tensor(0.2355, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 490\n",
      "episode bad predi 8.0\n",
      "Avg bad predi 8.22\n",
      "episode rew -8.0\n",
      "Avg Rew -8.52\n",
      "Loss tensor(0.2040, grad_fn=<SmoothL1LossBackward0>)\n",
      "\n",
      "game 500\n",
      "episode bad predi 6.0\n",
      "Avg bad predi 8.34\n",
      "episode rew -2.0\n",
      "Avg Rew -8.56\n",
      "Loss tensor(0.2041, grad_fn=<SmoothL1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Main Training Loop\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "player_opt = OptimalPlayer(epsilon=0.7, player='O')\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=5e-4)\n",
    "rew_buffer = deque([0.0], maxlen=50)\n",
    "bad_pred_buffer = deque([0.0], maxlen=50)\n",
    "\n",
    "game = 0\n",
    "old_game = 0\n",
    "step = 0\n",
    "bad_prediction = False\n",
    "\n",
    "episode_reward = 0.0\n",
    "episode_bad_predict = 0.0\n",
    "while(game < 500):\n",
    "    step +=1\n",
    "    \n",
    "   \n",
    "    #random agent play\n",
    "    \n",
    "    move = policy_net.act(obs)\n",
    "    obs = env.observe()[0]\n",
    "    available_action = find_aval_actions(obs, list_of_action)\n",
    "    \n",
    "    if not move in available_action:\n",
    "        #print(' bad prediction', available_action)\n",
    "        bad_prediction = True\n",
    "        \n",
    "    obs = env.observe()[0].flatten()\n",
    "    #print('move prediction', move)\n",
    "    #env.render()\n",
    "    #print('move', move)\n",
    "    \n",
    "    if not bad_prediction:\n",
    "        #print('env step')\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        #env.render()\n",
    "        \n",
    "    action = move\n",
    "    #print('###### action type #####',type(action))\n",
    "    #env.render()\n",
    "    \n",
    "    if (not end) and (not bad_prediction):\n",
    "        #optimal player play\n",
    "        move = player_opt.act(grid)\n",
    "        #print('optimal move', move)\n",
    "        \n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        #env.render()\n",
    "    \n",
    "\n",
    "    #transition set-off\n",
    "    new_obs = env.observe()[0].flatten()\n",
    "    if not bad_prediction:\n",
    "        rew = env.reward(player='X')\n",
    "    else:\n",
    "        rew = -1\n",
    "    transition = (obs, action, rew, end, new_obs)\n",
    "    #print('transition', transition)\n",
    "    replay_buffer.append(transition)\n",
    "    \n",
    "    \n",
    "    if end or bad_prediction:\n",
    "        #print('game finish')\n",
    "        if bad_prediction:\n",
    "            episode_bad_predict += 1\n",
    "            \n",
    "        bad_prediction = False\n",
    "        #print('-------------------------------------------')\n",
    "        #print('Game end, winner is player ' + str(winner))\n",
    "        #print('Optimal player = ' +  Turns[0])\n",
    "        #print('Random player = ' +  Turns[1])\n",
    "        #env.render()\n",
    "        env.reset()\n",
    "        episode_reward += rew\n",
    "        game += 1\n",
    "        if game % 10 == 0:\n",
    "            \n",
    "            rew_buffer.append(episode_reward)\n",
    "            bad_pred_buffer.append(episode_bad_predict)\n",
    "\n",
    "            # Logging\n",
    "            if game % 10 == 0:\n",
    "                print()\n",
    "                print('game', game)\n",
    "                print('episode bad predi',episode_bad_predict)\n",
    "                print('Avg bad predi', np.mean(bad_pred_buffer))\n",
    "                print('episode rew',episode_reward)\n",
    "                print('Avg Rew', np.mean(rew_buffer)) \n",
    "                print('Loss', loss)\n",
    "            episode_reward = 0.0\n",
    "            episode_bad_predict = 0.0\n",
    "    \n",
    "    \n",
    "    # Start Gradient Step\n",
    "    #print('replay_buffer', len(replay_buffer))\n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    \n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    #print('obsas', type(obses))\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    #print('actions type', type(actions))\n",
    "    #print(actions)\n",
    "    rews = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "    \n",
    "    obses_tensor = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    actions_tensor = torch.as_tensor(actions, dtype=torch.int64)#.unsqueeze(-1)\n",
    "    rews_tensor = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_tensor = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    #print('dones tensor', dones_tensor.shape)\n",
    "    new_obses_tensor = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_obses_tensor)\n",
    "    #print('target_q_values',target_q_values.shape)\n",
    "    mask_possible_action = torch.ne(new_obses_tensor,0)*-1\n",
    "    #print('mask_possible_action',mask_possible_action.shape)\n",
    "    possible_target_q_values = target_q_values * mask_possible_action\n",
    "    #print('possible_target_q_values',possible_target_q_values.shape)\n",
    "    max_target_q_values = possible_target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    #print('max_target_q_values', max_target_q_values.shape)\n",
    "    \n",
    "    targets = rews_tensor + GAMMA * (1 - dones_tensor) * max_target_q_values\n",
    "    #print('targets', targets.shape)\n",
    "    \n",
    "    \n",
    "    # Compute Loss\n",
    "    \n",
    "    q_values = policy_net(obses_tensor)\n",
    "    q_values_max = q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    \n",
    "    #print('q_values',q_values.shape,'q_values_max',q_values_max.shape)\n",
    "    \n",
    "    #action_q_values = torch.gather(input=q_values, dim=1, index=actions_tensor)\n",
    "    \n",
    "    #print('action_q_values',action_q_values.shape, 'targets',targets.shape)\n",
    "    \n",
    "    loss = nn.functional.smooth_l1_loss(q_values_max, targets)\n",
    "    \n",
    "    #print('loss', loss, loss.shape, type(loss))\n",
    "#     max_q_idx = torch.argmax(possible_q_values, dim=1)[0]\n",
    "#     max_q_idx_item = max_q_idx.detach().item()\n",
    "    \n",
    "    \n",
    "    # Gradient Descent\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    # Logging\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
    "print(obs_tensor.shape, obs_tensor)\n",
    "output_target = target_net(obs_tensor.unsqueeze(0))\n",
    "\n",
    "\n",
    "print(output_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_aval_actions(state_of_game,act):\n",
    "  possible_actions = []\n",
    "  for i in range(9):\n",
    "    if state_of_game[act[i]] == 0:\n",
    "      possible_actions.append(i)\n",
    "  return possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "obs = env.observe()[0]\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  0.],\n",
       "        [-1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "obs = env.observe()[0]\n",
    "available_action = find_aval_actions(obs, list_of_action)\n",
    "print(available_action)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
